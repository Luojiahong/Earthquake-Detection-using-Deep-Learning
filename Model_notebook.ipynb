{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju2TlHoybhZp",
        "colab_type": "text"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D8hMEAG7HHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew, kurtosis \n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from copy import deepcopy\n",
        "from time import time\n",
        "\n",
        "import sys\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "import torch.optim as optim\n",
        "from google.colab import files\n",
        "import os \n",
        "from torchvision import transforms\n",
        "\n",
        "import xgboost as xgb\n",
        "import sklearn.metrics \n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint as sp_randint, uniform\n",
        "\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thv2QQmFecLR",
        "colab_type": "code",
        "outputId": "9dd292ab-7e00-4067-c25f-122d34a5621d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "GPU = True\n",
        "device_idx = 0\n",
        "use_cuda = True\n",
        "\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhepmQxca89u",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOApr0griA9t",
        "colab_type": "code",
        "outputId": "9b042f6d-9098-40d4-b4b2-abf27771f3b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#sys.path.insert(0, \"gdrive/My Drive/Earthquakes/LayerGenerator.py\")\n",
        "from LayerGenerator import *"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wgkMfNLflJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"gdrive/My Drive/Earthquakes/train.csv\"\n",
        "file = open(path,\"r\")\n",
        "files = [open(\"gdrive/My Drive/Earthquakes/Split_Files/tempfile.part.0\"+str(i),\"r\") for i in range(7)]\n",
        "paths = [\"gdrive/My Drive/Earthquakes/Split_Files/tempfile.part.0\"+str(i) for i in range(7)]\n",
        "\n",
        "rows = 629145480"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIUeYvGlm5U_",
        "colab_type": "text"
      },
      "source": [
        "# CNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S5MhO6OIczq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import model class\n",
        "from CNN_model import CNN_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wubijCWb7s8o",
        "colab_type": "code",
        "outputId": "05f8ba69-88c5-4ab7-b2d0-3f1b332e20bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# intialise the model and run a dummy input through it as a quick bug check \n",
        "x = torch.randint(-15, 15, (5,1,150000)).float()  \n",
        "model = CNN_model()\n",
        "print(model(x))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0000],\n",
            "        [0.0634],\n",
            "        [0.0542],\n",
            "        [0.0000],\n",
            "        [0.2513]], grad_fn=<ReluBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU4ynx_lv4W0",
        "colab_type": "text"
      },
      "source": [
        "#Training Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbiSv49EwBH3",
        "colab_type": "text"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZogBbF3gCGyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(d):\n",
        "    print(\"Loading training data... \", end=\"\")\n",
        "    start_time = time()\n",
        "    \n",
        "    if d[\"start\"]==0 or d[\"start\"]==1:\n",
        "        data = pd.read_csv(d[\"file\"], skiprows = d[\"start\"], nrows = 50000000, names = [\"X\", \"y\"])\n",
        "    else:\n",
        "        data = pd.read_csv(d[\"file\"], skiprows = d[\"start\"], names = [\"X\", \"y\"])\n",
        "    print(\"Done! (time: {:.1f}s)\".format(time()-start_time))\n",
        "    return data\n",
        "\n",
        "    return data_new\n",
        "\n",
        "def sample_batch(data, batch_size):\n",
        "    unit = 150000\n",
        "  \n",
        "    n = data.shape[0]\n",
        "    batches_X = np.empty((batch_size,unit))\n",
        "    batches_y = np.empty((batch_size,1))\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        start = np.random.randint(0, n-unit-1)\n",
        "        obs = data.iloc[start:start+unit]\n",
        "\n",
        "        X = obs.X\n",
        "        y = obs.y.iloc[-1]\n",
        "        del obs\n",
        "        \n",
        "        #Normalize the data (train)\n",
        "        mean = X.mean(axis=0)\n",
        "        std = X.std(axis=0)\n",
        "        X = (X - mean)/std\n",
        "            \n",
        "        batches_X[i,:] = X\n",
        "        batches_y[i,:] = y\n",
        "    X = torch.from_numpy(np.array(batches_X)).float()\n",
        "    y = torch.from_numpy(np.array(batches_y)).float()\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GCP7oqfwJP7",
        "colab_type": "text"
      },
      "source": [
        "## Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmlBu82Hvpv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(inputs, targets, model, optimizer):\n",
        "\n",
        "    start = time()\n",
        "\n",
        "    #Re-start optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #Do forward pass\n",
        "    inputs = inputs.view(-1,1,unit).to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Computes loss on batch:\n",
        "    loss = F.l1_loss(outputs, targets)\n",
        "\n",
        "    # Do backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    #Optimizer updates model parameters using computed gradient.\n",
        "    optimizer.step()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return loss.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joSHMJ8hoWTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model):\n",
        "\n",
        "    print(\"Loading validation data... \", end=\"\")\n",
        "    \n",
        "    X, y, _ = load_data(file, 6000000, 6000000+1500001, 1500000, 0)\n",
        "    y_idx = np.array(range(150000-1,y.shape[0],150000))\n",
        "    y = y[y_idx]\n",
        "    \n",
        "    print(\"Done!\")\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X.view(-1,1,150000))\n",
        "        val_loss += F.l1_loss(outputs.view(-1), y, reduction=\"sum\").item()\n",
        "\n",
        "    val_loss /= 10\n",
        "\n",
        "    print(\"Validation set: Average loss: {:.4f}\\n\".format(val_loss))\n",
        "    return val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxx5y3O0yA93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_round(d_load,model,optimizer,batch_size,batches,interval):\n",
        "    #Load data\n",
        "    data = load_data(d_load)\n",
        "    \n",
        "    losses = []\n",
        "    \n",
        "    cum_loss = 0\n",
        "    c=0\n",
        "    start = time()\n",
        "    start_batch = 0\n",
        "    \n",
        "    for batch_id in range(batches):\n",
        "        #Get batch\n",
        "        inputs, targets = sample_batch(data,batch_size)\n",
        "        \n",
        "        #Train on the batch\n",
        "        loss = train(inputs, targets, model, optimizer)\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        cum_loss += loss.item()\n",
        "        c+=1\n",
        "        \n",
        "        if (batch_id+1)%interval==0:\n",
        "            print(\"Batches: {:0>3d} to {:0>3d} -- Loss: {:.4f} -- Time: {:.2f}s\".format(start_batch+1,batch_id+1,round(cum_loss/c,4),time()-start))\n",
        "            cum_loss = c = 0\n",
        "            start = time()\n",
        "            start_batch = batch_id+1\n",
        "    \n",
        "    del data\n",
        "    \n",
        "    return(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeqteYJnlgks",
        "colab_type": "text"
      },
      "source": [
        "# Training Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElzLgqPEwfUG",
        "colab_type": "text"
      },
      "source": [
        "Pseudo-code:\n",
        "1. Load 10% of data to RAM.\n",
        "2. Total random batches = enough to cover all data (balance between batch size and batches)\n",
        "Train - accepts batch, does training round, returns loss\n",
        "3. Re-mount to drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0qyjSSoyIdZ",
        "colab_type": "text"
      },
      "source": [
        "## Training set-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Xvf7WnyIE-",
        "colab_type": "code",
        "outputId": "3523a761-0c97-4870-ca32-76967a84d4dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch_size = 16\n",
        "batches = 128\n",
        "interval = 16\n",
        "\n",
        "model_save_name = \"CNN v3.pt\"\n",
        "path = F\"gdrive/My Drive/Earthquakes/{model_save_name}\"\n",
        "\n",
        "model = CNN_model().to(device)\n",
        "torch.save(model.state_dict(),path)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.1)\n",
        "\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total number of parameters is: {:,}\".format(params))\n",
        "\n",
        "ds = [{\n",
        "    \"file_no\": i//2,\n",
        "    \"start\": (i%2)*50000000\n",
        "} for i in range(13)]\n",
        "\n",
        "ds[0][\"start\"]=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of parameters is: 16,843,361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoRpMzAX29Gs",
        "colab_type": "code",
        "outputId": "5e7bc09d-b7bf-4d07-e1e5-7fbbcfa59324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15079
        }
      },
      "source": [
        "unit = 150000\n",
        "losses = []\n",
        "for epoch in range(10):\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    for i, d_load in enumerate(ds):\n",
        "        print(\"\\n~~~~~~~~ Training round {}.{} ~~~~~~~~\".format(epoch+1,i+1))\n",
        "        files = [open(\"gdrive/My Drive/Earthquakes/Split_Files/tempfile.part.0\"+str(i),\"r\") for i in range(7)]\n",
        "        d_load[\"file\"] = files[d_load[\"file_no\"]]\n",
        "        l = train_round(d_load,model,optimizer,batch_size,batches,interval)\n",
        "        losses.append(l)\n",
        "    torch.save(model.state_dict(),path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "~~~~~~~~ Training round 1.1 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.4s)\n",
            "Batches: 001 to 016 -- Loss: 5.0000 -- Time: 11.74s\n",
            "Batches: 017 to 032 -- Loss: 4.4933 -- Time: 11.82s\n",
            "Batches: 033 to 048 -- Loss: 4.1864 -- Time: 11.81s\n",
            "Batches: 049 to 064 -- Loss: 3.9123 -- Time: 11.84s\n",
            "Batches: 065 to 080 -- Loss: 3.6099 -- Time: 11.70s\n",
            "Batches: 081 to 096 -- Loss: 3.4740 -- Time: 11.71s\n",
            "Batches: 097 to 112 -- Loss: 3.1198 -- Time: 11.65s\n",
            "Batches: 113 to 128 -- Loss: 3.4828 -- Time: 11.61s\n",
            "\n",
            "~~~~~~~~ Training round 1.2 ~~~~~~~~\n",
            "Loading training data... Done! (time: 15.9s)\n",
            "Batches: 001 to 016 -- Loss: 4.7239 -- Time: 11.71s\n",
            "Batches: 017 to 032 -- Loss: 4.0131 -- Time: 11.77s\n",
            "Batches: 033 to 048 -- Loss: 3.7858 -- Time: 11.85s\n",
            "Batches: 049 to 064 -- Loss: 3.1187 -- Time: 12.02s\n",
            "Batches: 065 to 080 -- Loss: 3.1535 -- Time: 11.75s\n",
            "Batches: 081 to 096 -- Loss: 2.6458 -- Time: 11.67s\n",
            "Batches: 097 to 112 -- Loss: 3.1060 -- Time: 11.64s\n",
            "Batches: 113 to 128 -- Loss: 2.5523 -- Time: 11.66s\n",
            "\n",
            "~~~~~~~~ Training round 1.3 ~~~~~~~~\n",
            "Loading training data... Done! (time: 12.7s)\n",
            "Batches: 001 to 016 -- Loss: 2.5755 -- Time: 11.74s\n",
            "Batches: 017 to 032 -- Loss: 2.2808 -- Time: 11.84s\n",
            "Batches: 033 to 048 -- Loss: 2.2970 -- Time: 11.86s\n",
            "Batches: 049 to 064 -- Loss: 2.3947 -- Time: 11.83s\n",
            "Batches: 065 to 080 -- Loss: 2.1931 -- Time: 11.80s\n",
            "Batches: 081 to 096 -- Loss: 2.3084 -- Time: 11.65s\n",
            "Batches: 097 to 112 -- Loss: 2.1869 -- Time: 11.62s\n",
            "Batches: 113 to 128 -- Loss: 2.2016 -- Time: 11.69s\n",
            "\n",
            "~~~~~~~~ Training round 1.4 ~~~~~~~~\n",
            "Loading training data... Done! (time: 18.7s)\n",
            "Batches: 001 to 016 -- Loss: 2.2665 -- Time: 11.75s\n",
            "Batches: 017 to 032 -- Loss: 2.0192 -- Time: 11.77s\n",
            "Batches: 033 to 048 -- Loss: 1.7917 -- Time: 11.90s\n",
            "Batches: 049 to 064 -- Loss: 1.7370 -- Time: 11.86s\n",
            "Batches: 065 to 080 -- Loss: 1.8299 -- Time: 11.76s\n",
            "Batches: 081 to 096 -- Loss: 1.7804 -- Time: 11.78s\n",
            "Batches: 097 to 112 -- Loss: 1.7051 -- Time: 11.65s\n",
            "Batches: 113 to 128 -- Loss: 1.7522 -- Time: 11.65s\n",
            "\n",
            "~~~~~~~~ Training round 1.5 ~~~~~~~~\n",
            "Loading training data... Done! (time: 12.8s)\n",
            "Batches: 001 to 016 -- Loss: 2.7306 -- Time: 11.75s\n",
            "Batches: 017 to 032 -- Loss: 2.1046 -- Time: 11.79s\n",
            "Batches: 033 to 048 -- Loss: 1.7478 -- Time: 11.96s\n",
            "Batches: 049 to 064 -- Loss: 1.9434 -- Time: 12.00s\n",
            "Batches: 065 to 080 -- Loss: 2.6291 -- Time: 11.76s\n",
            "Batches: 081 to 096 -- Loss: 2.1281 -- Time: 11.70s\n",
            "Batches: 097 to 112 -- Loss: 1.9985 -- Time: 11.65s\n",
            "Batches: 113 to 128 -- Loss: 2.2237 -- Time: 11.65s\n",
            "\n",
            "~~~~~~~~ Training round 1.6 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.0s)\n",
            "Batches: 001 to 016 -- Loss: 4.8470 -- Time: 11.78s\n",
            "Batches: 017 to 032 -- Loss: 4.1418 -- Time: 11.79s\n",
            "Batches: 033 to 048 -- Loss: 3.1310 -- Time: 11.95s\n",
            "Batches: 049 to 064 -- Loss: 2.8046 -- Time: 11.85s\n",
            "Batches: 065 to 080 -- Loss: 2.7739 -- Time: 11.73s\n",
            "Batches: 081 to 096 -- Loss: 2.3557 -- Time: 11.66s\n",
            "Batches: 097 to 112 -- Loss: 2.6429 -- Time: 11.64s\n",
            "Batches: 113 to 128 -- Loss: 2.4404 -- Time: 11.76s\n",
            "\n",
            "~~~~~~~~ Training round 1.7 ~~~~~~~~\n",
            "Loading training data... Done! (time: 12.9s)\n",
            "Batches: 001 to 016 -- Loss: 3.1724 -- Time: 11.67s\n",
            "Batches: 017 to 032 -- Loss: 2.1901 -- Time: 11.73s\n",
            "Batches: 033 to 048 -- Loss: 1.6200 -- Time: 11.89s\n",
            "Batches: 049 to 064 -- Loss: 1.4936 -- Time: 11.82s\n",
            "Batches: 065 to 080 -- Loss: 1.5332 -- Time: 11.76s\n",
            "Batches: 081 to 096 -- Loss: 1.5261 -- Time: 11.72s\n",
            "Batches: 097 to 112 -- Loss: 1.4585 -- Time: 11.66s\n",
            "Batches: 113 to 128 -- Loss: 1.5163 -- Time: 11.60s\n",
            "\n",
            "~~~~~~~~ Training round 1.8 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.8s)\n",
            "Batches: 001 to 016 -- Loss: 1.9913 -- Time: 11.70s\n",
            "Batches: 017 to 032 -- Loss: 2.0689 -- Time: 11.74s\n",
            "Batches: 033 to 048 -- Loss: 1.9745 -- Time: 11.95s\n",
            "Batches: 049 to 064 -- Loss: 1.8407 -- Time: 11.82s\n",
            "Batches: 065 to 080 -- Loss: 1.7373 -- Time: 11.69s\n",
            "Batches: 081 to 096 -- Loss: 1.6214 -- Time: 11.60s\n",
            "Batches: 097 to 112 -- Loss: 1.7956 -- Time: 11.60s\n",
            "Batches: 113 to 128 -- Loss: 1.5425 -- Time: 11.59s\n",
            "\n",
            "~~~~~~~~ Training round 1.9 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.9s)\n",
            "Batches: 001 to 016 -- Loss: 1.4221 -- Time: 11.78s\n",
            "Batches: 017 to 032 -- Loss: 1.5488 -- Time: 11.73s\n",
            "Batches: 033 to 048 -- Loss: 1.5238 -- Time: 11.82s\n",
            "Batches: 049 to 064 -- Loss: 1.5251 -- Time: 11.75s\n",
            "Batches: 065 to 080 -- Loss: 1.3768 -- Time: 11.69s\n",
            "Batches: 081 to 096 -- Loss: 1.6697 -- Time: 11.69s\n",
            "Batches: 097 to 112 -- Loss: 1.5005 -- Time: 11.73s\n",
            "Batches: 113 to 128 -- Loss: 1.4996 -- Time: 11.62s\n",
            "\n",
            "~~~~~~~~ Training round 1.10 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.1s)\n",
            "Batches: 001 to 016 -- Loss: 1.5024 -- Time: 11.61s\n",
            "Batches: 017 to 032 -- Loss: 1.3746 -- Time: 11.67s\n",
            "Batches: 033 to 048 -- Loss: 1.0347 -- Time: 11.80s\n",
            "Batches: 049 to 064 -- Loss: 1.1449 -- Time: 11.84s\n",
            "Batches: 065 to 080 -- Loss: 1.2122 -- Time: 11.72s\n",
            "Batches: 081 to 096 -- Loss: 1.1497 -- Time: 11.59s\n",
            "Batches: 097 to 112 -- Loss: 1.0749 -- Time: 11.53s\n",
            "Batches: 113 to 128 -- Loss: 1.1706 -- Time: 11.55s\n",
            "\n",
            "~~~~~~~~ Training round 1.11 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.2s)\n",
            "Batches: 001 to 016 -- Loss: 3.7898 -- Time: 11.60s\n",
            "Batches: 017 to 032 -- Loss: 3.6404 -- Time: 11.66s\n",
            "Batches: 033 to 048 -- Loss: 3.5732 -- Time: 11.78s\n",
            "Batches: 049 to 064 -- Loss: 3.3722 -- Time: 11.65s\n",
            "Batches: 065 to 080 -- Loss: 3.5067 -- Time: 11.58s\n",
            "Batches: 081 to 096 -- Loss: 3.4233 -- Time: 11.54s\n",
            "Batches: 097 to 112 -- Loss: 3.4110 -- Time: 11.49s\n",
            "Batches: 113 to 128 -- Loss: 3.3938 -- Time: 11.52s\n",
            "\n",
            "~~~~~~~~ Training round 1.12 ~~~~~~~~\n",
            "Loading training data... Done! (time: 19.4s)\n",
            "Batches: 001 to 016 -- Loss: 1.9428 -- Time: 11.57s\n",
            "Batches: 017 to 032 -- Loss: 1.7710 -- Time: 11.57s\n",
            "Batches: 033 to 048 -- Loss: 1.8306 -- Time: 11.73s\n",
            "Batches: 049 to 064 -- Loss: 1.8467 -- Time: 11.66s\n",
            "Batches: 065 to 080 -- Loss: 1.8153 -- Time: 11.71s\n",
            "Batches: 081 to 096 -- Loss: 2.0669 -- Time: 11.52s\n",
            "Batches: 097 to 112 -- Loss: 1.9475 -- Time: 11.57s\n",
            "Batches: 113 to 128 -- Loss: 2.0702 -- Time: 11.51s\n",
            "\n",
            "~~~~~~~~ Training round 1.13 ~~~~~~~~\n",
            "Loading training data... Done! (time: 7.8s)\n",
            "Batches: 001 to 016 -- Loss: 2.1374 -- Time: 12.18s\n",
            "Batches: 017 to 032 -- Loss: 1.8903 -- Time: 11.70s\n",
            "Batches: 033 to 048 -- Loss: 1.8000 -- Time: 12.20s\n",
            "Batches: 049 to 064 -- Loss: 1.8767 -- Time: 11.88s\n",
            "Batches: 065 to 080 -- Loss: 1.8020 -- Time: 11.97s\n",
            "Batches: 081 to 096 -- Loss: 1.7398 -- Time: 11.78s\n",
            "Batches: 097 to 112 -- Loss: 1.6747 -- Time: 11.98s\n",
            "Batches: 113 to 128 -- Loss: 1.7220 -- Time: 11.84s\n",
            "\n",
            "~~~~~~~~ Training round 2.1 ~~~~~~~~\n",
            "Loading training data... Done! (time: 14.6s)\n",
            "Batches: 001 to 016 -- Loss: 2.1316 -- Time: 12.24s\n",
            "Batches: 017 to 032 -- Loss: 2.2129 -- Time: 11.75s\n",
            "Batches: 033 to 048 -- Loss: 1.8730 -- Time: 12.04s\n",
            "Batches: 049 to 064 -- Loss: 1.9640 -- Time: 12.30s\n",
            "Batches: 065 to 080 -- Loss: 1.9783 -- Time: 12.22s\n",
            "Batches: 081 to 096 -- Loss: 2.0600 -- Time: 12.22s\n",
            "Batches: 097 to 112 -- Loss: 1.8578 -- Time: 12.20s\n",
            "Batches: 113 to 128 -- Loss: 2.0395 -- Time: 12.18s\n",
            "\n",
            "~~~~~~~~ Training round 2.2 ~~~~~~~~\n",
            "Loading training data... Done! (time: 18.3s)\n",
            "Batches: 001 to 016 -- Loss: 2.7556 -- Time: 12.26s\n",
            "Batches: 017 to 032 -- Loss: 2.2133 -- Time: 12.09s\n",
            "Batches: 033 to 048 -- Loss: 2.0351 -- Time: 12.19s\n",
            "Batches: 049 to 064 -- Loss: 1.7801 -- Time: 12.16s\n",
            "Batches: 065 to 080 -- Loss: 1.9211 -- Time: 12.08s\n",
            "Batches: 081 to 096 -- Loss: 1.7292 -- Time: 12.21s\n",
            "Batches: 097 to 112 -- Loss: 1.9016 -- Time: 12.10s\n",
            "Batches: 113 to 128 -- Loss: 1.7237 -- Time: 12.09s\n",
            "\n",
            "~~~~~~~~ Training round 2.3 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.2s)\n",
            "Batches: 001 to 016 -- Loss: 2.4418 -- Time: 12.08s\n",
            "Batches: 017 to 032 -- Loss: 2.1249 -- Time: 12.03s\n",
            "Batches: 033 to 048 -- Loss: 1.8841 -- Time: 12.12s\n",
            "Batches: 049 to 064 -- Loss: 1.6513 -- Time: 12.05s\n",
            "Batches: 065 to 080 -- Loss: 1.7637 -- Time: 12.11s\n",
            "Batches: 081 to 096 -- Loss: 1.8566 -- Time: 11.70s\n",
            "Batches: 097 to 112 -- Loss: 2.0741 -- Time: 11.98s\n",
            "Batches: 113 to 128 -- Loss: 1.8328 -- Time: 11.72s\n",
            "\n",
            "~~~~~~~~ Training round 2.4 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.4s)\n",
            "Batches: 001 to 016 -- Loss: 1.7623 -- Time: 12.06s\n",
            "Batches: 017 to 032 -- Loss: 1.6256 -- Time: 11.95s\n",
            "Batches: 033 to 048 -- Loss: 1.7201 -- Time: 11.95s\n",
            "Batches: 049 to 064 -- Loss: 1.7320 -- Time: 12.06s\n",
            "Batches: 065 to 080 -- Loss: 1.6339 -- Time: 12.01s\n",
            "Batches: 081 to 096 -- Loss: 1.6669 -- Time: 11.98s\n",
            "Batches: 097 to 112 -- Loss: 1.7552 -- Time: 11.99s\n",
            "Batches: 113 to 128 -- Loss: 1.6905 -- Time: 12.05s\n",
            "\n",
            "~~~~~~~~ Training round 2.5 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.4s)\n",
            "Batches: 001 to 016 -- Loss: 2.3222 -- Time: 12.06s\n",
            "Batches: 017 to 032 -- Loss: 1.8439 -- Time: 11.97s\n",
            "Batches: 033 to 048 -- Loss: 1.7345 -- Time: 11.96s\n",
            "Batches: 049 to 064 -- Loss: 1.7342 -- Time: 11.96s\n",
            "Batches: 065 to 080 -- Loss: 1.6866 -- Time: 12.04s\n",
            "Batches: 081 to 096 -- Loss: 1.4315 -- Time: 11.96s\n",
            "Batches: 097 to 112 -- Loss: 1.9720 -- Time: 11.95s\n",
            "Batches: 113 to 128 -- Loss: 1.6052 -- Time: 11.94s\n",
            "\n",
            "~~~~~~~~ Training round 2.6 ~~~~~~~~\n",
            "Loading training data... Done! (time: 18.0s)\n",
            "Batches: 001 to 016 -- Loss: 5.3483 -- Time: 11.89s\n",
            "Batches: 017 to 032 -- Loss: 4.1424 -- Time: 11.89s\n",
            "Batches: 033 to 048 -- Loss: 3.4356 -- Time: 11.85s\n",
            "Batches: 049 to 064 -- Loss: 2.6743 -- Time: 11.88s\n",
            "Batches: 065 to 080 -- Loss: 2.3796 -- Time: 11.86s\n",
            "Batches: 081 to 096 -- Loss: 2.2391 -- Time: 11.91s\n",
            "Batches: 097 to 112 -- Loss: 2.3268 -- Time: 11.88s\n",
            "Batches: 113 to 128 -- Loss: 2.3156 -- Time: 11.95s\n",
            "\n",
            "~~~~~~~~ Training round 2.7 ~~~~~~~~\n",
            "Loading training data... Done! (time: 14.5s)\n",
            "Batches: 001 to 016 -- Loss: 3.0874 -- Time: 11.90s\n",
            "Batches: 017 to 032 -- Loss: 1.9571 -- Time: 11.91s\n",
            "Batches: 033 to 048 -- Loss: 1.3220 -- Time: 11.85s\n",
            "Batches: 049 to 064 -- Loss: 1.4435 -- Time: 11.86s\n",
            "Batches: 065 to 080 -- Loss: 1.4373 -- Time: 11.85s\n",
            "Batches: 081 to 096 -- Loss: 1.2767 -- Time: 11.90s\n",
            "Batches: 097 to 112 -- Loss: 1.3837 -- Time: 11.95s\n",
            "Batches: 113 to 128 -- Loss: 1.1888 -- Time: 11.91s\n",
            "\n",
            "~~~~~~~~ Training round 2.8 ~~~~~~~~\n",
            "Loading training data... Done! (time: 18.5s)\n",
            "Batches: 001 to 016 -- Loss: 1.7257 -- Time: 11.92s\n",
            "Batches: 017 to 032 -- Loss: 1.7813 -- Time: 11.80s\n",
            "Batches: 033 to 048 -- Loss: 1.7741 -- Time: 11.87s\n",
            "Batches: 049 to 064 -- Loss: 1.6238 -- Time: 11.90s\n",
            "Batches: 065 to 080 -- Loss: 1.4913 -- Time: 11.83s\n",
            "Batches: 081 to 096 -- Loss: 1.4956 -- Time: 11.82s\n",
            "Batches: 097 to 112 -- Loss: 1.6556 -- Time: 11.85s\n",
            "Batches: 113 to 128 -- Loss: 1.8269 -- Time: 11.84s\n",
            "\n",
            "~~~~~~~~ Training round 2.9 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.3s)\n",
            "Batches: 001 to 016 -- Loss: 1.4949 -- Time: 11.87s\n",
            "Batches: 017 to 032 -- Loss: 1.5930 -- Time: 11.86s\n",
            "Batches: 033 to 048 -- Loss: 1.4254 -- Time: 11.88s\n",
            "Batches: 049 to 064 -- Loss: 1.4055 -- Time: 11.81s\n",
            "Batches: 065 to 080 -- Loss: 1.6008 -- Time: 11.89s\n",
            "Batches: 081 to 096 -- Loss: 1.3969 -- Time: 11.87s\n",
            "Batches: 097 to 112 -- Loss: 1.3879 -- Time: 11.87s\n",
            "Batches: 113 to 128 -- Loss: 1.3393 -- Time: 11.90s\n",
            "\n",
            "~~~~~~~~ Training round 2.10 ~~~~~~~~\n",
            "Loading training data... Done! (time: 18.3s)\n",
            "Batches: 001 to 016 -- Loss: 1.3056 -- Time: 11.83s\n",
            "Batches: 017 to 032 -- Loss: 1.1354 -- Time: 11.75s\n",
            "Batches: 033 to 048 -- Loss: 1.0653 -- Time: 11.81s\n",
            "Batches: 049 to 064 -- Loss: 1.1529 -- Time: 11.80s\n",
            "Batches: 065 to 080 -- Loss: 1.2943 -- Time: 11.83s\n",
            "Batches: 081 to 096 -- Loss: 1.1874 -- Time: 11.98s\n",
            "Batches: 097 to 112 -- Loss: 1.2531 -- Time: 11.95s\n",
            "Batches: 113 to 128 -- Loss: 1.1437 -- Time: 11.86s\n",
            "\n",
            "~~~~~~~~ Training round 2.11 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.1s)\n",
            "Batches: 001 to 016 -- Loss: 3.4842 -- Time: 11.87s\n",
            "Batches: 017 to 032 -- Loss: 3.4689 -- Time: 11.75s\n",
            "Batches: 033 to 048 -- Loss: 3.0794 -- Time: 11.90s\n",
            "Batches: 049 to 064 -- Loss: 3.1267 -- Time: 11.81s\n",
            "Batches: 065 to 080 -- Loss: 3.2178 -- Time: 11.85s\n",
            "Batches: 081 to 096 -- Loss: 3.2692 -- Time: 11.85s\n",
            "Batches: 097 to 112 -- Loss: 2.9374 -- Time: 11.88s\n",
            "Batches: 113 to 128 -- Loss: 3.2876 -- Time: 11.88s\n",
            "\n",
            "~~~~~~~~ Training round 2.12 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.6s)\n",
            "Batches: 001 to 016 -- Loss: 2.0837 -- Time: 11.90s\n",
            "Batches: 017 to 032 -- Loss: 1.7867 -- Time: 11.80s\n",
            "Batches: 033 to 048 -- Loss: 1.6553 -- Time: 11.79s\n",
            "Batches: 049 to 064 -- Loss: 1.6891 -- Time: 11.78s\n",
            "Batches: 065 to 080 -- Loss: 1.8460 -- Time: 11.84s\n",
            "Batches: 081 to 096 -- Loss: 1.9071 -- Time: 11.83s\n",
            "Batches: 097 to 112 -- Loss: 1.7802 -- Time: 11.95s\n",
            "Batches: 113 to 128 -- Loss: 1.6842 -- Time: 11.87s\n",
            "\n",
            "~~~~~~~~ Training round 2.13 ~~~~~~~~\n",
            "Loading training data... Done! (time: 7.7s)\n",
            "Batches: 001 to 016 -- Loss: 1.9925 -- Time: 11.83s\n",
            "Batches: 017 to 032 -- Loss: 1.9305 -- Time: 11.70s\n",
            "Batches: 033 to 048 -- Loss: 1.8958 -- Time: 11.71s\n",
            "Batches: 049 to 064 -- Loss: 1.8458 -- Time: 11.75s\n",
            "Batches: 065 to 080 -- Loss: 1.8457 -- Time: 11.79s\n",
            "Batches: 081 to 096 -- Loss: 1.5597 -- Time: 11.88s\n",
            "Batches: 097 to 112 -- Loss: 1.4340 -- Time: 11.78s\n",
            "Batches: 113 to 128 -- Loss: 1.6439 -- Time: 11.75s\n",
            "\n",
            "~~~~~~~~ Training round 3.1 ~~~~~~~~\n",
            "Loading training data... Done! (time: 14.2s)\n",
            "Batches: 001 to 016 -- Loss: 1.8490 -- Time: 11.83s\n",
            "Batches: 017 to 032 -- Loss: 1.8240 -- Time: 11.82s\n",
            "Batches: 033 to 048 -- Loss: 1.9064 -- Time: 11.92s\n",
            "Batches: 049 to 064 -- Loss: 1.8686 -- Time: 11.82s\n",
            "Batches: 065 to 080 -- Loss: 1.7776 -- Time: 11.80s\n",
            "Batches: 081 to 096 -- Loss: 1.8806 -- Time: 11.83s\n",
            "Batches: 097 to 112 -- Loss: 1.7545 -- Time: 11.81s\n",
            "Batches: 113 to 128 -- Loss: 1.8907 -- Time: 11.85s\n",
            "\n",
            "~~~~~~~~ Training round 3.2 ~~~~~~~~\n",
            "Loading training data... Done! (time: 18.6s)\n",
            "Batches: 001 to 016 -- Loss: 2.7646 -- Time: 11.87s\n",
            "Batches: 017 to 032 -- Loss: 2.1932 -- Time: 11.72s\n",
            "Batches: 033 to 048 -- Loss: 2.0721 -- Time: 11.74s\n",
            "Batches: 049 to 064 -- Loss: 1.8634 -- Time: 11.76s\n",
            "Batches: 065 to 080 -- Loss: 1.6428 -- Time: 11.86s\n",
            "Batches: 081 to 096 -- Loss: 1.9575 -- Time: 11.76s\n",
            "Batches: 097 to 112 -- Loss: 1.9030 -- Time: 11.90s\n",
            "Batches: 113 to 128 -- Loss: 1.7743 -- Time: 11.81s\n",
            "\n",
            "~~~~~~~~ Training round 3.3 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.1s)\n",
            "Batches: 001 to 016 -- Loss: 2.3595 -- Time: 11.83s\n",
            "Batches: 017 to 032 -- Loss: 2.1644 -- Time: 11.78s\n",
            "Batches: 033 to 048 -- Loss: 1.8839 -- Time: 11.79s\n",
            "Batches: 049 to 064 -- Loss: 1.8720 -- Time: 11.99s\n",
            "Batches: 065 to 080 -- Loss: 1.5517 -- Time: 11.91s\n",
            "Batches: 081 to 096 -- Loss: 1.7900 -- Time: 11.88s\n",
            "Batches: 097 to 112 -- Loss: 1.8539 -- Time: 11.80s\n",
            "Batches: 113 to 128 -- Loss: 2.0582 -- Time: 11.81s\n",
            "\n",
            "~~~~~~~~ Training round 3.4 ~~~~~~~~\n",
            "Loading training data... Done! (time: 16.7s)\n",
            "Batches: 001 to 016 -- Loss: 1.7930 -- Time: 11.84s\n",
            "Batches: 017 to 032 -- Loss: 1.5471 -- Time: 11.87s\n",
            "Batches: 033 to 048 -- Loss: 1.6363 -- Time: 11.80s\n",
            "Batches: 049 to 064 -- Loss: 1.7822 -- Time: 11.76s\n",
            "Batches: 065 to 080 -- Loss: 1.7157 -- Time: 11.77s\n",
            "Batches: 081 to 096 -- Loss: 1.6729 -- Time: 11.80s\n",
            "Batches: 097 to 112 -- Loss: 1.6932 -- Time: 11.78s\n",
            "Batches: 113 to 128 -- Loss: 1.5404 -- Time: 11.86s\n",
            "\n",
            "~~~~~~~~ Training round 3.5 ~~~~~~~~\n",
            "Loading training data... Done! (time: 14.3s)\n",
            "Batches: 001 to 016 -- Loss: 2.0843 -- Time: 11.84s\n",
            "Batches: 017 to 032 -- Loss: 1.9500 -- Time: 11.73s\n",
            "Batches: 033 to 048 -- Loss: 1.7858 -- Time: 11.86s\n",
            "Batches: 049 to 064 -- Loss: 1.8955 -- Time: 11.79s\n",
            "Batches: 065 to 080 -- Loss: 2.1446 -- Time: 11.79s\n",
            "Batches: 081 to 096 -- Loss: 1.5750 -- Time: 11.89s\n",
            "Batches: 097 to 112 -- Loss: 1.3941 -- Time: 11.90s\n",
            "Batches: 113 to 128 -- Loss: 1.7484 -- Time: 11.80s\n",
            "\n",
            "~~~~~~~~ Training round 3.6 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.6s)\n",
            "Batches: 001 to 016 -- Loss: 5.0239 -- Time: 11.81s\n",
            "Batches: 017 to 032 -- Loss: 4.2921 -- Time: 11.83s\n",
            "Batches: 033 to 048 -- Loss: 3.3552 -- Time: 11.78s\n",
            "Batches: 049 to 064 -- Loss: 2.8789 -- Time: 11.91s\n",
            "Batches: 065 to 080 -- Loss: 2.3894 -- Time: 11.74s\n",
            "Batches: 081 to 096 -- Loss: 2.0780 -- Time: 11.71s\n",
            "Batches: 097 to 112 -- Loss: 2.2556 -- Time: 11.78s\n",
            "Batches: 113 to 128 -- Loss: 2.0347 -- Time: 11.80s\n",
            "\n",
            "~~~~~~~~ Training round 3.7 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.3s)\n",
            "Batches: 001 to 016 -- Loss: 3.3009 -- Time: 11.83s\n",
            "Batches: 017 to 032 -- Loss: 1.7486 -- Time: 11.77s\n",
            "Batches: 033 to 048 -- Loss: 1.4791 -- Time: 11.68s\n",
            "Batches: 049 to 064 -- Loss: 1.1484 -- Time: 11.77s\n",
            "Batches: 065 to 080 -- Loss: 1.3081 -- Time: 11.79s\n",
            "Batches: 081 to 096 -- Loss: 1.2152 -- Time: 11.86s\n",
            "Batches: 097 to 112 -- Loss: 1.2331 -- Time: 11.79s\n",
            "Batches: 113 to 128 -- Loss: 1.3811 -- Time: 11.89s\n",
            "\n",
            "~~~~~~~~ Training round 3.8 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.5s)\n",
            "Batches: 001 to 016 -- Loss: 2.1312 -- Time: 11.93s\n",
            "Batches: 017 to 032 -- Loss: 1.8376 -- Time: 11.75s\n",
            "Batches: 033 to 048 -- Loss: 1.8721 -- Time: 11.80s\n",
            "Batches: 049 to 064 -- Loss: 1.3502 -- Time: 11.81s\n",
            "Batches: 065 to 080 -- Loss: 1.5488 -- Time: 11.92s\n",
            "Batches: 081 to 096 -- Loss: 1.6081 -- Time: 11.89s\n",
            "Batches: 097 to 112 -- Loss: 1.4112 -- Time: 11.77s\n",
            "Batches: 113 to 128 -- Loss: 1.4580 -- Time: 11.82s\n",
            "\n",
            "~~~~~~~~ Training round 3.9 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.7s)\n",
            "Batches: 001 to 016 -- Loss: 1.3786 -- Time: 11.98s\n",
            "Batches: 017 to 032 -- Loss: 1.4566 -- Time: 11.76s\n",
            "Batches: 033 to 048 -- Loss: 1.4054 -- Time: 11.84s\n",
            "Batches: 049 to 064 -- Loss: 1.5314 -- Time: 11.79s\n",
            "Batches: 065 to 080 -- Loss: 1.5490 -- Time: 11.80s\n",
            "Batches: 081 to 096 -- Loss: 1.4340 -- Time: 11.79s\n",
            "Batches: 097 to 112 -- Loss: 1.3762 -- Time: 11.80s\n",
            "Batches: 113 to 128 -- Loss: 1.3957 -- Time: 11.81s\n",
            "\n",
            "~~~~~~~~ Training round 3.10 ~~~~~~~~\n",
            "Loading training data... Done! (time: 18.4s)\n",
            "Batches: 001 to 016 -- Loss: 1.3499 -- Time: 11.84s\n",
            "Batches: 017 to 032 -- Loss: 1.0654 -- Time: 11.79s\n",
            "Batches: 033 to 048 -- Loss: 1.2227 -- Time: 11.82s\n",
            "Batches: 049 to 064 -- Loss: 1.0172 -- Time: 11.81s\n",
            "Batches: 065 to 080 -- Loss: 1.2366 -- Time: 11.83s\n",
            "Batches: 081 to 096 -- Loss: 1.0898 -- Time: 11.80s\n",
            "Batches: 097 to 112 -- Loss: 1.0349 -- Time: 11.89s\n",
            "Batches: 113 to 128 -- Loss: 1.0563 -- Time: 11.88s\n",
            "\n",
            "~~~~~~~~ Training round 3.11 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.2s)\n",
            "Batches: 001 to 016 -- Loss: 3.6761 -- Time: 11.82s\n",
            "Batches: 017 to 032 -- Loss: 3.4989 -- Time: 11.74s\n",
            "Batches: 033 to 048 -- Loss: 3.3621 -- Time: 11.81s\n",
            "Batches: 049 to 064 -- Loss: 3.2494 -- Time: 11.79s\n",
            "Batches: 065 to 080 -- Loss: 3.2367 -- Time: 11.88s\n",
            "Batches: 081 to 096 -- Loss: 3.4066 -- Time: 11.84s\n",
            "Batches: 097 to 112 -- Loss: 3.2600 -- Time: 11.86s\n",
            "Batches: 113 to 128 -- Loss: 3.2145 -- Time: 11.92s\n",
            "\n",
            "~~~~~~~~ Training round 3.12 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.1s)\n",
            "Batches: 001 to 016 -- Loss: 1.9517 -- Time: 11.89s\n",
            "Batches: 017 to 032 -- Loss: 1.7889 -- Time: 11.77s\n",
            "Batches: 033 to 048 -- Loss: 1.9400 -- Time: 11.82s\n",
            "Batches: 049 to 064 -- Loss: 1.8514 -- Time: 11.80s\n",
            "Batches: 065 to 080 -- Loss: 1.6670 -- Time: 11.74s\n",
            "Batches: 081 to 096 -- Loss: 1.7003 -- Time: 11.74s\n",
            "Batches: 097 to 112 -- Loss: 1.9815 -- Time: 11.73s\n",
            "Batches: 113 to 128 -- Loss: 1.8987 -- Time: 11.78s\n",
            "\n",
            "~~~~~~~~ Training round 3.13 ~~~~~~~~\n",
            "Loading training data... Done! (time: 8.2s)\n",
            "Batches: 001 to 016 -- Loss: 2.2289 -- Time: 11.84s\n",
            "Batches: 017 to 032 -- Loss: 2.0572 -- Time: 11.65s\n",
            "Batches: 033 to 048 -- Loss: 2.0349 -- Time: 11.72s\n",
            "Batches: 049 to 064 -- Loss: 1.7934 -- Time: 11.73s\n",
            "Batches: 065 to 080 -- Loss: 1.7510 -- Time: 11.68s\n",
            "Batches: 081 to 096 -- Loss: 1.6679 -- Time: 11.76s\n",
            "Batches: 097 to 112 -- Loss: 1.5076 -- Time: 11.81s\n",
            "Batches: 113 to 128 -- Loss: 1.6557 -- Time: 11.81s\n",
            "\n",
            "~~~~~~~~ Training round 4.1 ~~~~~~~~\n",
            "Loading training data... Done! (time: 14.5s)\n",
            "Batches: 001 to 016 -- Loss: 2.1741 -- Time: 11.89s\n",
            "Batches: 017 to 032 -- Loss: 1.9887 -- Time: 11.78s\n",
            "Batches: 033 to 048 -- Loss: 1.6356 -- Time: 11.79s\n",
            "Batches: 049 to 064 -- Loss: 1.8839 -- Time: 11.74s\n",
            "Batches: 065 to 080 -- Loss: 1.8014 -- Time: 11.89s\n",
            "Batches: 081 to 096 -- Loss: 1.8688 -- Time: 11.82s\n",
            "Batches: 097 to 112 -- Loss: 2.1144 -- Time: 11.98s\n",
            "Batches: 113 to 128 -- Loss: 1.7334 -- Time: 11.89s\n",
            "\n",
            "~~~~~~~~ Training round 4.2 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.8s)\n",
            "Batches: 001 to 016 -- Loss: 2.8920 -- Time: 11.84s\n",
            "Batches: 017 to 032 -- Loss: 2.2421 -- Time: 11.84s\n",
            "Batches: 033 to 048 -- Loss: 1.8871 -- Time: 11.93s\n",
            "Batches: 049 to 064 -- Loss: 1.7490 -- Time: 11.81s\n",
            "Batches: 065 to 080 -- Loss: 1.7659 -- Time: 11.80s\n",
            "Batches: 081 to 096 -- Loss: 1.9834 -- Time: 11.80s\n",
            "Batches: 097 to 112 -- Loss: 1.7194 -- Time: 11.78s\n",
            "Batches: 113 to 128 -- Loss: 1.8580 -- Time: 11.83s\n",
            "\n",
            "~~~~~~~~ Training round 4.3 ~~~~~~~~\n",
            "Loading training data... Done! (time: 14.4s)\n",
            "Batches: 001 to 016 -- Loss: 2.4039 -- Time: 11.83s\n",
            "Batches: 017 to 032 -- Loss: 1.9831 -- Time: 11.75s\n",
            "Batches: 033 to 048 -- Loss: 1.8500 -- Time: 11.75s\n",
            "Batches: 049 to 064 -- Loss: 1.8597 -- Time: 11.81s\n",
            "Batches: 065 to 080 -- Loss: 1.7196 -- Time: 11.85s\n",
            "Batches: 081 to 096 -- Loss: 1.7534 -- Time: 11.87s\n",
            "Batches: 097 to 112 -- Loss: 1.6845 -- Time: 11.89s\n",
            "Batches: 113 to 128 -- Loss: 1.9494 -- Time: 11.83s\n",
            "\n",
            "~~~~~~~~ Training round 4.4 ~~~~~~~~\n",
            "Loading training data... Done! (time: 16.9s)\n",
            "Batches: 001 to 016 -- Loss: 1.8676 -- Time: 11.81s\n",
            "Batches: 017 to 032 -- Loss: 1.5250 -- Time: 11.78s\n",
            "Batches: 033 to 048 -- Loss: 1.7135 -- Time: 11.77s\n",
            "Batches: 049 to 064 -- Loss: 1.5404 -- Time: 11.91s\n",
            "Batches: 065 to 080 -- Loss: 1.5054 -- Time: 11.93s\n",
            "Batches: 081 to 096 -- Loss: 1.5331 -- Time: 11.85s\n",
            "Batches: 097 to 112 -- Loss: 1.6335 -- Time: 11.86s\n",
            "Batches: 113 to 128 -- Loss: 1.5526 -- Time: 11.84s\n",
            "\n",
            "~~~~~~~~ Training round 4.5 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.2s)\n",
            "Batches: 001 to 016 -- Loss: 2.2753 -- Time: 11.90s\n",
            "Batches: 017 to 032 -- Loss: 1.9142 -- Time: 11.82s\n",
            "Batches: 033 to 048 -- Loss: 1.9168 -- Time: 11.79s\n",
            "Batches: 049 to 064 -- Loss: 2.0307 -- Time: 11.82s\n",
            "Batches: 065 to 080 -- Loss: 2.0669 -- Time: 11.80s\n",
            "Batches: 081 to 096 -- Loss: 1.7304 -- Time: 11.83s\n",
            "Batches: 097 to 112 -- Loss: 2.1562 -- Time: 11.87s\n",
            "Batches: 113 to 128 -- Loss: 1.8095 -- Time: 11.88s\n",
            "\n",
            "~~~~~~~~ Training round 4.6 ~~~~~~~~\n",
            "Loading training data... Done! (time: 18.0s)\n",
            "Batches: 001 to 016 -- Loss: 5.1202 -- Time: 11.83s\n",
            "Batches: 017 to 032 -- Loss: 4.5275 -- Time: 11.78s\n",
            "Batches: 033 to 048 -- Loss: 3.4706 -- Time: 11.74s\n",
            "Batches: 049 to 064 -- Loss: 3.2101 -- Time: 11.83s\n",
            "Batches: 065 to 080 -- Loss: 2.5062 -- Time: 11.84s\n",
            "Batches: 081 to 096 -- Loss: 2.3821 -- Time: 11.88s\n",
            "Batches: 097 to 112 -- Loss: 2.1617 -- Time: 11.73s\n",
            "Batches: 113 to 128 -- Loss: 2.2876 -- Time: 11.75s\n",
            "\n",
            "~~~~~~~~ Training round 4.7 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.2s)\n",
            "Batches: 001 to 016 -- Loss: 2.9856 -- Time: 11.89s\n",
            "Batches: 017 to 032 -- Loss: 1.8624 -- Time: 11.79s\n",
            "Batches: 033 to 048 -- Loss: 1.6508 -- Time: 11.88s\n",
            "Batches: 049 to 064 -- Loss: 1.4279 -- Time: 11.93s\n",
            "Batches: 065 to 080 -- Loss: 1.4371 -- Time: 11.81s\n",
            "Batches: 081 to 096 -- Loss: 1.2770 -- Time: 11.82s\n",
            "Batches: 097 to 112 -- Loss: 1.3614 -- Time: 11.87s\n",
            "Batches: 113 to 128 -- Loss: 1.3133 -- Time: 11.79s\n",
            "\n",
            "~~~~~~~~ Training round 4.8 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.2s)\n",
            "Batches: 001 to 016 -- Loss: 2.0167 -- Time: 11.87s\n",
            "Batches: 017 to 032 -- Loss: 1.8822 -- Time: 11.76s\n",
            "Batches: 033 to 048 -- Loss: 1.6975 -- Time: 11.76s\n",
            "Batches: 049 to 064 -- Loss: 1.6398 -- Time: 11.82s\n",
            "Batches: 065 to 080 -- Loss: 1.4799 -- Time: 11.84s\n",
            "Batches: 081 to 096 -- Loss: 1.4046 -- Time: 11.79s\n",
            "Batches: 097 to 112 -- Loss: 1.5653 -- Time: 11.85s\n",
            "Batches: 113 to 128 -- Loss: 1.7147 -- Time: 11.89s\n",
            "\n",
            "~~~~~~~~ Training round 4.9 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.8s)\n",
            "Batches: 001 to 016 -- Loss: 1.4686 -- Time: 11.96s\n",
            "Batches: 017 to 032 -- Loss: 1.3895 -- Time: 11.80s\n",
            "Batches: 033 to 048 -- Loss: 1.3353 -- Time: 11.78s\n",
            "Batches: 049 to 064 -- Loss: 1.4167 -- Time: 11.82s\n",
            "Batches: 065 to 080 -- Loss: 1.2650 -- Time: 11.95s\n",
            "Batches: 081 to 096 -- Loss: 1.4890 -- Time: 11.84s\n",
            "Batches: 097 to 112 -- Loss: 1.2769 -- Time: 11.78s\n",
            "Batches: 113 to 128 -- Loss: 1.4561 -- Time: 11.84s\n",
            "\n",
            "~~~~~~~~ Training round 4.10 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.8s)\n",
            "Batches: 001 to 016 -- Loss: 1.4049 -- Time: 11.83s\n",
            "Batches: 017 to 032 -- Loss: 1.2461 -- Time: 11.92s\n",
            "Batches: 033 to 048 -- Loss: 1.2824 -- Time: 11.78s\n",
            "Batches: 049 to 064 -- Loss: 1.1332 -- Time: 11.75s\n",
            "Batches: 065 to 080 -- Loss: 1.4455 -- Time: 11.79s\n",
            "Batches: 081 to 096 -- Loss: 1.4012 -- Time: 11.80s\n",
            "Batches: 097 to 112 -- Loss: 1.1429 -- Time: 11.80s\n",
            "Batches: 113 to 128 -- Loss: 0.9886 -- Time: 11.83s\n",
            "\n",
            "~~~~~~~~ Training round 4.11 ~~~~~~~~\n",
            "Loading training data... Done! (time: 14.2s)\n",
            "Batches: 001 to 016 -- Loss: 4.2812 -- Time: 11.82s\n",
            "Batches: 017 to 032 -- Loss: 3.5986 -- Time: 11.78s\n",
            "Batches: 033 to 048 -- Loss: 3.1939 -- Time: 11.81s\n",
            "Batches: 049 to 064 -- Loss: 3.3776 -- Time: 11.77s\n",
            "Batches: 065 to 080 -- Loss: 3.1911 -- Time: 11.79s\n",
            "Batches: 081 to 096 -- Loss: 3.1585 -- Time: 11.86s\n",
            "Batches: 097 to 112 -- Loss: 3.2743 -- Time: 11.93s\n",
            "Batches: 113 to 128 -- Loss: 3.3695 -- Time: 11.76s\n",
            "\n",
            "~~~~~~~~ Training round 4.12 ~~~~~~~~\n",
            "Loading training data... Done! (time: 16.9s)\n",
            "Batches: 001 to 016 -- Loss: 1.7459 -- Time: 11.85s\n",
            "Batches: 017 to 032 -- Loss: 1.9801 -- Time: 11.75s\n",
            "Batches: 033 to 048 -- Loss: 1.8289 -- Time: 11.78s\n",
            "Batches: 049 to 064 -- Loss: 1.7018 -- Time: 11.91s\n",
            "Batches: 065 to 080 -- Loss: 1.7743 -- Time: 11.77s\n",
            "Batches: 081 to 096 -- Loss: 1.7126 -- Time: 11.79s\n",
            "Batches: 097 to 112 -- Loss: 1.7838 -- Time: 11.85s\n",
            "Batches: 113 to 128 -- Loss: 1.8874 -- Time: 11.85s\n",
            "\n",
            "~~~~~~~~ Training round 4.13 ~~~~~~~~\n",
            "Loading training data... Done! (time: 8.0s)\n",
            "Batches: 001 to 016 -- Loss: 2.1831 -- Time: 11.92s\n",
            "Batches: 017 to 032 -- Loss: 1.8669 -- Time: 11.84s\n",
            "Batches: 033 to 048 -- Loss: 1.8872 -- Time: 11.74s\n",
            "Batches: 049 to 064 -- Loss: 1.9238 -- Time: 11.76s\n",
            "Batches: 065 to 080 -- Loss: 1.6050 -- Time: 11.77s\n",
            "Batches: 081 to 096 -- Loss: 1.6843 -- Time: 11.81s\n",
            "Batches: 097 to 112 -- Loss: 1.8434 -- Time: 11.79s\n",
            "Batches: 113 to 128 -- Loss: 1.4138 -- Time: 11.79s\n",
            "\n",
            "~~~~~~~~ Training round 5.1 ~~~~~~~~\n",
            "Loading training data... Done! (time: 15.2s)\n",
            "Batches: 001 to 016 -- Loss: 2.1930 -- Time: 12.02s\n",
            "Batches: 017 to 032 -- Loss: 2.1963 -- Time: 11.93s\n",
            "Batches: 033 to 048 -- Loss: 2.1288 -- Time: 11.97s\n",
            "Batches: 049 to 064 -- Loss: 2.1806 -- Time: 11.94s\n",
            "Batches: 065 to 080 -- Loss: 2.1623 -- Time: 12.03s\n",
            "Batches: 081 to 096 -- Loss: 1.8019 -- Time: 12.03s\n",
            "Batches: 097 to 112 -- Loss: 1.9100 -- Time: 12.05s\n",
            "Batches: 113 to 128 -- Loss: 1.9437 -- Time: 12.01s\n",
            "\n",
            "~~~~~~~~ Training round 5.2 ~~~~~~~~\n",
            "Loading training data... Done! (time: 16.8s)\n",
            "Batches: 001 to 016 -- Loss: 2.6647 -- Time: 12.01s\n",
            "Batches: 017 to 032 -- Loss: 2.3462 -- Time: 11.93s\n",
            "Batches: 033 to 048 -- Loss: 1.9754 -- Time: 11.95s\n",
            "Batches: 049 to 064 -- Loss: 2.0368 -- Time: 12.08s\n",
            "Batches: 065 to 080 -- Loss: 1.8621 -- Time: 12.00s\n",
            "Batches: 081 to 096 -- Loss: 1.8087 -- Time: 12.02s\n",
            "Batches: 097 to 112 -- Loss: 1.8924 -- Time: 12.01s\n",
            "Batches: 113 to 128 -- Loss: 1.6985 -- Time: 12.15s\n",
            "\n",
            "~~~~~~~~ Training round 5.3 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.4s)\n",
            "Batches: 001 to 016 -- Loss: 2.4611 -- Time: 12.10s\n",
            "Batches: 017 to 032 -- Loss: 2.3194 -- Time: 11.95s\n",
            "Batches: 033 to 048 -- Loss: 1.8299 -- Time: 11.93s\n",
            "Batches: 049 to 064 -- Loss: 1.9342 -- Time: 11.93s\n",
            "Batches: 065 to 080 -- Loss: 1.7319 -- Time: 11.97s\n",
            "Batches: 081 to 096 -- Loss: 1.8110 -- Time: 11.97s\n",
            "Batches: 097 to 112 -- Loss: 1.9899 -- Time: 12.03s\n",
            "Batches: 113 to 128 -- Loss: 1.8921 -- Time: 12.16s\n",
            "\n",
            "~~~~~~~~ Training round 5.4 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.0s)\n",
            "Batches: 001 to 016 -- Loss: 1.6880 -- Time: 12.02s\n",
            "Batches: 017 to 032 -- Loss: 1.6651 -- Time: 12.11s\n",
            "Batches: 033 to 048 -- Loss: 1.6084 -- Time: 12.02s\n",
            "Batches: 049 to 064 -- Loss: 1.6109 -- Time: 11.93s\n",
            "Batches: 065 to 080 -- Loss: 1.6091 -- Time: 12.09s\n",
            "Batches: 081 to 096 -- Loss: 1.5715 -- Time: 12.02s\n",
            "Batches: 097 to 112 -- Loss: 1.6441 -- Time: 12.03s\n",
            "Batches: 113 to 128 -- Loss: 1.6598 -- Time: 12.03s\n",
            "\n",
            "~~~~~~~~ Training round 5.5 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.1s)\n",
            "Batches: 001 to 016 -- Loss: 2.0133 -- Time: 12.08s\n",
            "Batches: 017 to 032 -- Loss: 1.8290 -- Time: 11.99s\n",
            "Batches: 033 to 048 -- Loss: 1.7564 -- Time: 12.10s\n",
            "Batches: 049 to 064 -- Loss: 1.5788 -- Time: 12.01s\n",
            "Batches: 065 to 080 -- Loss: 1.7322 -- Time: 12.04s\n",
            "Batches: 081 to 096 -- Loss: 1.9274 -- Time: 12.17s\n",
            "Batches: 097 to 112 -- Loss: 1.9685 -- Time: 11.96s\n",
            "Batches: 113 to 128 -- Loss: 1.5137 -- Time: 12.06s\n",
            "\n",
            "~~~~~~~~ Training round 5.6 ~~~~~~~~\n",
            "Loading training data... Done! (time: 18.3s)\n",
            "Batches: 001 to 016 -- Loss: 4.7235 -- Time: 12.05s\n",
            "Batches: 017 to 032 -- Loss: 4.6843 -- Time: 11.93s\n",
            "Batches: 033 to 048 -- Loss: 3.7893 -- Time: 12.00s\n",
            "Batches: 049 to 064 -- Loss: 3.0952 -- Time: 11.94s\n",
            "Batches: 065 to 080 -- Loss: 2.8844 -- Time: 12.00s\n",
            "Batches: 081 to 096 -- Loss: 2.5160 -- Time: 12.07s\n",
            "Batches: 097 to 112 -- Loss: 2.1502 -- Time: 12.12s\n",
            "Batches: 113 to 128 -- Loss: 2.2854 -- Time: 12.00s\n",
            "\n",
            "~~~~~~~~ Training round 5.7 ~~~~~~~~\n",
            "Loading training data... Done! (time: 14.1s)\n",
            "Batches: 001 to 016 -- Loss: 3.0557 -- Time: 12.05s\n",
            "Batches: 017 to 032 -- Loss: 2.1981 -- Time: 11.99s\n",
            "Batches: 033 to 048 -- Loss: 1.5621 -- Time: 12.01s\n",
            "Batches: 049 to 064 -- Loss: 1.2680 -- Time: 12.13s\n",
            "Batches: 065 to 080 -- Loss: 1.2656 -- Time: 12.06s\n",
            "Batches: 081 to 096 -- Loss: 1.3828 -- Time: 12.09s\n",
            "Batches: 097 to 112 -- Loss: 1.2917 -- Time: 12.06s\n",
            "Batches: 113 to 128 -- Loss: 1.2806 -- Time: 12.06s\n",
            "\n",
            "~~~~~~~~ Training round 5.8 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.1s)\n",
            "Batches: 001 to 016 -- Loss: 1.9444 -- Time: 12.16s\n",
            "Batches: 017 to 032 -- Loss: 1.9198 -- Time: 12.03s\n",
            "Batches: 033 to 048 -- Loss: 1.8107 -- Time: 12.06s\n",
            "Batches: 049 to 064 -- Loss: 1.6878 -- Time: 12.13s\n",
            "Batches: 065 to 080 -- Loss: 1.4372 -- Time: 12.02s\n",
            "Batches: 081 to 096 -- Loss: 1.5300 -- Time: 12.07s\n",
            "Batches: 097 to 112 -- Loss: 1.5805 -- Time: 12.08s\n",
            "Batches: 113 to 128 -- Loss: 1.4967 -- Time: 12.20s\n",
            "\n",
            "~~~~~~~~ Training round 5.9 ~~~~~~~~\n",
            "Loading training data... Done! (time: 12.4s)\n",
            "Batches: 001 to 016 -- Loss: 1.3628 -- Time: 12.12s\n",
            "Batches: 017 to 032 -- Loss: 1.4719 -- Time: 12.04s\n",
            "Batches: 033 to 048 -- Loss: 1.4425 -- Time: 12.08s\n",
            "Batches: 049 to 064 -- Loss: 1.4603 -- Time: 12.06s\n",
            "Batches: 065 to 080 -- Loss: 1.4292 -- Time: 12.13s\n",
            "Batches: 081 to 096 -- Loss: 1.3575 -- Time: 12.18s\n",
            "Batches: 097 to 112 -- Loss: 1.2610 -- Time: 12.21s\n",
            "Batches: 113 to 128 -- Loss: 1.5109 -- Time: 12.09s\n",
            "\n",
            "~~~~~~~~ Training round 5.10 ~~~~~~~~\n",
            "Loading training data... Done! (time: 16.9s)\n",
            "Batches: 001 to 016 -- Loss: 1.3904 -- Time: 12.15s\n",
            "Batches: 017 to 032 -- Loss: 1.1570 -- Time: 12.02s\n",
            "Batches: 033 to 048 -- Loss: 1.0774 -- Time: 12.17s\n",
            "Batches: 049 to 064 -- Loss: 1.2288 -- Time: 12.01s\n",
            "Batches: 065 to 080 -- Loss: 1.0804 -- Time: 12.02s\n",
            "Batches: 081 to 096 -- Loss: 1.1392 -- Time: 12.05s\n",
            "Batches: 097 to 112 -- Loss: 1.0233 -- Time: 12.07s\n",
            "Batches: 113 to 128 -- Loss: 1.0700 -- Time: 12.09s\n",
            "\n",
            "~~~~~~~~ Training round 5.11 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.9s)\n",
            "Batches: 001 to 016 -- Loss: 3.8550 -- Time: 12.17s\n",
            "Batches: 017 to 032 -- Loss: 3.7540 -- Time: 12.14s\n",
            "Batches: 033 to 048 -- Loss: 3.4800 -- Time: 12.04s\n",
            "Batches: 049 to 064 -- Loss: 3.7520 -- Time: 12.06s\n",
            "Batches: 065 to 080 -- Loss: 3.5117 -- Time: 12.08s\n",
            "Batches: 081 to 096 -- Loss: 3.1809 -- Time: 12.09s\n",
            "Batches: 097 to 112 -- Loss: 3.3144 -- Time: 12.18s\n",
            "Batches: 113 to 128 -- Loss: 3.3527 -- Time: 12.10s\n",
            "\n",
            "~~~~~~~~ Training round 5.12 ~~~~~~~~\n",
            "Loading training data... Done! (time: 16.8s)\n",
            "Batches: 001 to 016 -- Loss: 1.8674 -- Time: 12.11s\n",
            "Batches: 017 to 032 -- Loss: 1.9072 -- Time: 12.02s\n",
            "Batches: 033 to 048 -- Loss: 1.9732 -- Time: 12.08s\n",
            "Batches: 049 to 064 -- Loss: 1.8699 -- Time: 12.16s\n",
            "Batches: 065 to 080 -- Loss: 1.6756 -- Time: 12.12s\n",
            "Batches: 081 to 096 -- Loss: 1.9568 -- Time: 12.07s\n",
            "Batches: 097 to 112 -- Loss: 1.6793 -- Time: 12.07s\n",
            "Batches: 113 to 128 -- Loss: 1.8703 -- Time: 12.12s\n",
            "\n",
            "~~~~~~~~ Training round 5.13 ~~~~~~~~\n",
            "Loading training data... Done! (time: 7.3s)\n",
            "Batches: 001 to 016 -- Loss: 2.1393 -- Time: 12.11s\n",
            "Batches: 017 to 032 -- Loss: 1.9412 -- Time: 12.12s\n",
            "Batches: 033 to 048 -- Loss: 1.5384 -- Time: 12.08s\n",
            "Batches: 049 to 064 -- Loss: 1.7021 -- Time: 12.04s\n",
            "Batches: 065 to 080 -- Loss: 1.7060 -- Time: 12.06s\n",
            "Batches: 081 to 096 -- Loss: 1.6430 -- Time: 12.11s\n",
            "Batches: 097 to 112 -- Loss: 1.6896 -- Time: 12.11s\n",
            "Batches: 113 to 128 -- Loss: 1.4972 -- Time: 12.06s\n",
            "\n",
            "~~~~~~~~ Training round 6.1 ~~~~~~~~\n",
            "Loading training data... Done! (time: 15.7s)\n",
            "Batches: 001 to 016 -- Loss: 1.7650 -- Time: 12.12s\n",
            "Batches: 017 to 032 -- Loss: 2.1280 -- Time: 12.10s\n",
            "Batches: 033 to 048 -- Loss: 2.0753 -- Time: 12.12s\n",
            "Batches: 049 to 064 -- Loss: 1.8844 -- Time: 12.10s\n",
            "Batches: 065 to 080 -- Loss: 1.9391 -- Time: 12.08s\n",
            "Batches: 081 to 096 -- Loss: 1.9858 -- Time: 12.19s\n",
            "Batches: 097 to 112 -- Loss: 1.8923 -- Time: 12.09s\n",
            "Batches: 113 to 128 -- Loss: 1.8067 -- Time: 12.08s\n",
            "\n",
            "~~~~~~~~ Training round 6.2 ~~~~~~~~\n",
            "Loading training data... Done! (time: 16.8s)\n",
            "Batches: 001 to 016 -- Loss: 2.9708 -- Time: 12.10s\n",
            "Batches: 017 to 032 -- Loss: 2.2806 -- Time: 12.12s\n",
            "Batches: 033 to 048 -- Loss: 2.1006 -- Time: 12.14s\n",
            "Batches: 049 to 064 -- Loss: 2.0507 -- Time: 12.12s\n",
            "Batches: 065 to 080 -- Loss: 1.7703 -- Time: 12.11s\n",
            "Batches: 081 to 096 -- Loss: 2.0271 -- Time: 12.14s\n",
            "Batches: 097 to 112 -- Loss: 1.9290 -- Time: 12.11s\n",
            "Batches: 113 to 128 -- Loss: 1.6782 -- Time: 12.15s\n",
            "\n",
            "~~~~~~~~ Training round 6.3 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.2s)\n",
            "Batches: 001 to 016 -- Loss: 2.2041 -- Time: 12.21s\n",
            "Batches: 017 to 032 -- Loss: 1.9434 -- Time: 12.11s\n",
            "Batches: 033 to 048 -- Loss: 1.9864 -- Time: 12.08s\n",
            "Batches: 049 to 064 -- Loss: 2.0663 -- Time: 12.09s\n",
            "Batches: 065 to 080 -- Loss: 1.9924 -- Time: 12.08s\n",
            "Batches: 081 to 096 -- Loss: 1.9138 -- Time: 12.11s\n",
            "Batches: 097 to 112 -- Loss: 1.6528 -- Time: 12.24s\n",
            "Batches: 113 to 128 -- Loss: 1.6426 -- Time: 12.16s\n",
            "\n",
            "~~~~~~~~ Training round 6.4 ~~~~~~~~\n",
            "Loading training data... Done! (time: 16.9s)\n",
            "Batches: 001 to 016 -- Loss: 1.7860 -- Time: 12.12s\n",
            "Batches: 017 to 032 -- Loss: 1.6692 -- Time: 12.05s\n",
            "Batches: 033 to 048 -- Loss: 1.6232 -- Time: 12.09s\n",
            "Batches: 049 to 064 -- Loss: 1.6062 -- Time: 12.11s\n",
            "Batches: 065 to 080 -- Loss: 1.5933 -- Time: 12.18s\n",
            "Batches: 081 to 096 -- Loss: 1.8175 -- Time: 12.11s\n",
            "Batches: 097 to 112 -- Loss: 1.6566 -- Time: 12.13s\n",
            "Batches: 113 to 128 -- Loss: 1.5837 -- Time: 12.16s\n",
            "\n",
            "~~~~~~~~ Training round 6.5 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.2s)\n",
            "Batches: 001 to 016 -- Loss: 2.3817 -- Time: 12.15s\n",
            "Batches: 017 to 032 -- Loss: 1.9411 -- Time: 12.13s\n",
            "Batches: 033 to 048 -- Loss: 1.5894 -- Time: 12.16s\n",
            "Batches: 049 to 064 -- Loss: 1.7626 -- Time: 12.13s\n",
            "Batches: 065 to 080 -- Loss: 1.6934 -- Time: 12.14s\n",
            "Batches: 081 to 096 -- Loss: 1.7908 -- Time: 12.15s\n",
            "Batches: 097 to 112 -- Loss: 1.9242 -- Time: 12.11s\n",
            "Batches: 113 to 128 -- Loss: 1.6632 -- Time: 12.11s\n",
            "\n",
            "~~~~~~~~ Training round 6.6 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.6s)\n",
            "Batches: 001 to 016 -- Loss: 5.0041 -- Time: 12.16s\n",
            "Batches: 017 to 032 -- Loss: 4.2968 -- Time: 12.08s\n",
            "Batches: 033 to 048 -- Loss: 3.6954 -- Time: 12.09s\n",
            "Batches: 049 to 064 -- Loss: 3.4549 -- Time: 12.10s\n",
            "Batches: 065 to 080 -- Loss: 2.9581 -- Time: 12.21s\n",
            "Batches: 081 to 096 -- Loss: 2.5003 -- Time: 12.15s\n",
            "Batches: 097 to 112 -- Loss: 2.1913 -- Time: 12.17s\n",
            "Batches: 113 to 128 -- Loss: 2.1915 -- Time: 12.12s\n",
            "\n",
            "~~~~~~~~ Training round 6.7 ~~~~~~~~\n",
            "Loading training data... Done! (time: 13.0s)\n",
            "Batches: 001 to 016 -- Loss: 3.3473 -- Time: 12.15s\n",
            "Batches: 017 to 032 -- Loss: 2.2179 -- Time: 12.02s\n",
            "Batches: 033 to 048 -- Loss: 1.7284 -- Time: 12.10s\n",
            "Batches: 049 to 064 -- Loss: 1.4339 -- Time: 12.22s\n",
            "Batches: 065 to 080 -- Loss: 1.3316 -- Time: 12.13s\n",
            "Batches: 081 to 096 -- Loss: 1.3358 -- Time: 12.20s\n",
            "Batches: 097 to 112 -- Loss: 1.3084 -- Time: 12.12s\n",
            "Batches: 113 to 128 -- Loss: 1.2245 -- Time: 12.13s\n",
            "\n",
            "~~~~~~~~ Training round 6.8 ~~~~~~~~\n",
            "Loading training data... Done! (time: 16.4s)\n",
            "Batches: 001 to 016 -- Loss: 1.9729 -- Time: 12.24s\n",
            "Batches: 017 to 032 -- Loss: 1.7625 -- Time: 12.10s\n",
            "Batches: 033 to 048 -- Loss: 1.7223 -- Time: 12.06s\n",
            "Batches: 049 to 064 -- Loss: 1.4811 -- Time: 12.12s\n",
            "Batches: 065 to 080 -- Loss: 1.3867 -- Time: 12.15s\n",
            "Batches: 081 to 096 -- Loss: 1.4261 -- Time: 12.09s\n",
            "Batches: 097 to 112 -- Loss: 1.2972 -- Time: 12.14s\n",
            "Batches: 113 to 128 -- Loss: 1.3776 -- Time: 12.23s\n",
            "\n",
            "~~~~~~~~ Training round 6.9 ~~~~~~~~\n",
            "Loading training data... Done! (time: 12.8s)\n",
            "Batches: 001 to 016 -- Loss: 1.3729 -- Time: 12.18s\n",
            "Batches: 017 to 032 -- Loss: 1.3653 -- Time: 12.06s\n",
            "Batches: 033 to 048 -- Loss: 1.5281 -- Time: 12.23s\n",
            "Batches: 049 to 064 -- Loss: 1.3264 -- Time: 12.10s\n",
            "Batches: 065 to 080 -- Loss: 1.4639 -- Time: 12.22s\n",
            "Batches: 081 to 096 -- Loss: 1.5625 -- Time: 12.20s\n",
            "Batches: 097 to 112 -- Loss: 1.3400 -- Time: 12.16s\n",
            "Batches: 113 to 128 -- Loss: 1.3852 -- Time: 12.16s\n",
            "\n",
            "~~~~~~~~ Training round 6.10 ~~~~~~~~\n",
            "Loading training data... Done! (time: 16.8s)\n",
            "Batches: 001 to 016 -- Loss: 1.5124 -- Time: 12.12s\n",
            "Batches: 017 to 032 -- Loss: 1.2405 -- Time: 12.14s\n",
            "Batches: 033 to 048 -- Loss: 1.1214 -- Time: 12.24s\n",
            "Batches: 049 to 064 -- Loss: 1.1136 -- Time: 12.19s\n",
            "Batches: 065 to 080 -- Loss: 0.9818 -- Time: 12.11s\n",
            "Batches: 081 to 096 -- Loss: 1.0458 -- Time: 12.15s\n",
            "Batches: 097 to 112 -- Loss: 1.0847 -- Time: 12.16s\n",
            "Batches: 113 to 128 -- Loss: 1.0064 -- Time: 12.17s\n",
            "\n",
            "~~~~~~~~ Training round 6.11 ~~~~~~~~\n",
            "Loading training data... Done! (time: 14.1s)\n",
            "Batches: 001 to 016 -- Loss: 3.6929 -- Time: 12.17s\n",
            "Batches: 017 to 032 -- Loss: 3.7228 -- Time: 12.12s\n",
            "Batches: 033 to 048 -- Loss: 3.4649 -- Time: 12.07s\n",
            "Batches: 049 to 064 -- Loss: 3.2419 -- Time: 12.17s\n",
            "Batches: 065 to 080 -- Loss: 3.2026 -- Time: 12.15s\n",
            "Batches: 081 to 096 -- Loss: 3.2844 -- Time: 12.20s\n",
            "Batches: 097 to 112 -- Loss: 3.0649 -- Time: 12.25s\n",
            "Batches: 113 to 128 -- Loss: 3.2147 -- Time: 12.16s\n",
            "\n",
            "~~~~~~~~ Training round 6.12 ~~~~~~~~\n",
            "Loading training data... Done! (time: 17.2s)\n",
            "Batches: 001 to 016 -- Loss: 1.9399 -- Time: 12.23s\n",
            "Batches: 017 to 032 -- Loss: 1.7234 -- Time: 12.11s\n",
            "Batches: 033 to 048 -- Loss: 1.7571 -- Time: 12.12s\n",
            "Batches: 049 to 064 -- Loss: 1.7934 -- Time: 12.25s\n",
            "Batches: 065 to 080 -- Loss: 1.7410 -- Time: 12.23s\n",
            "Batches: 081 to 096 -- Loss: 1.8151 -- Time: 12.24s\n",
            "Batches: 097 to 112 -- Loss: 1.7668 -- Time: 12.22s\n",
            "Batches: 113 to 128 -- Loss: 1.8879 -- Time: 12.21s\n",
            "\n",
            "~~~~~~~~ Training round 6.13 ~~~~~~~~\n",
            "Loading training data... Done! (time: 7.4s)\n",
            "Batches: 001 to 016 -- Loss: 1.9373 -- Time: 12.29s\n",
            "Batches: 017 to 032 -- Loss: 2.1892 -- Time: 12.22s\n",
            "Batches: 033 to 048 -- Loss: 1.7396 -- Time: 12.17s\n",
            "Batches: 049 to 064 -- Loss: 1.7357 -- Time: 12.16s\n",
            "Batches: 065 to 080 -- Loss: 1.6153 -- Time: 12.18s\n",
            "Batches: 081 to 096 -- Loss: 1.7352 -- Time: 12.20s\n",
            "Batches: 097 to 112 -- Loss: 1.5585 -- Time: 12.17s\n",
            "Batches: 113 to 128 -- Loss: 1.6288 -- Time: 12.26s\n",
            "\n",
            "~~~~~~~~ Training round 7.1 ~~~~~~~~\n",
            "Loading training data... Done! (time: 14.2s)\n",
            "Batches: 001 to 016 -- Loss: 1.9074 -- Time: 12.20s\n",
            "Batches: 017 to 032 -- Loss: 1.8528 -- Time: 12.12s\n",
            "Batches: 033 to 048 -- Loss: 2.0660 -- Time: 12.19s\n",
            "Batches: 049 to 064 -- Loss: 1.9400 -- Time: 12.22s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-177e6afaccb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gdrive/My Drive/Earthquakes/Split_Files/tempfile.part.0\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0md_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_no\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-9d1cd72ddbc9>\u001b[0m in \u001b[0;36mtrain_round\u001b[0;34m(d_load, model, optimizer, batch_size, batches, interval)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#Train on the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e6aceeff3c64>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(inputs, targets, model, optimizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Do backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#Optimizer updates model parameters using computed gradient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3o_Oh2Pm5su",
        "colab_type": "text"
      },
      "source": [
        "## Comments and results\n",
        "\n",
        "The model works reasonably well but there are a some issues that need to be addressed: \n",
        "\n",
        "* Due to the extremely high volume of data and the import methodology, after being done with training on a tranche of the data the model seems to overfit on it - when moving to the next tranche the loss usually starts higher before dropping back to the long-term mean. This is a non-trivial issue and means another methodology of streaming the data to the model needs to be devised.\n",
        "\n",
        "* On some tranches the model does significantly better than on others, up to 3 times better. We need to visualise the data and model output to form an idea of why this is.\n",
        "\n",
        "* Validation is not yet implemented properly - while at this stage we're more interested in identifying a model with the explanatory capacity required to achieve the desired level of accuracy (even if it overfits), we will need to \n",
        "\n",
        "* The model would score somewhere in the middle on the Kaggle ranking with the resuts achieved here. We have implemented and trained better models (which will be added here after the competition ends), however the major challenge remains identifying a model with the capacity to model the process sufficiently well while still running on the limited processing power we have available using free resources."
      ]
    }
  ]
}